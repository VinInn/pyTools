{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Restrict training to one gpu\n",
    "import imp\n",
    "try:\n",
    "        imp.find_module('setGPU')\n",
    "        import setGPU\n",
    "except ImportError:\n",
    "        found = False\n",
    "#/////////////////////\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "K.set_session(sess)\n",
    "import pylab as P\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.callbacks\n",
    "import glob\n",
    "import math\n",
    "\n",
    "#Useful sklearn functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "#Keras model related imports\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,Conv1D,LSTM,Flatten,Dropout,Activation\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "#For training progress bar to display correctly\n",
    "#see: https://github.com/bstriner/keras-tqdm/blob/master/examples/keras_progress_bars.ipynb\n",
    "from keras_tqdm import TQDMNotebookCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#/////Reading the data\n",
    "#/////////////////////\n",
    "clus = np.genfromtxt(\"/Users/innocent/data/clusterShape.csv\", delimiter=\" \", skip_header=0, names=True)\n",
    "clus['l2'] = np.nan_to_num(clus['l2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('isBarrel', 'layer', 'simX', 'simY', 'simSX', 'simSY', 'recX', 'recY', 'x', 'y', 'xx', 'yy', 'xy', 'dx', 'dy', 'l2', 'sx', 'sy', 's', 'q')\n"
     ]
    }
   ],
   "source": [
    "n = clus.dtype.names\n",
    "print n\n",
    "n.index('isBarrel')\n",
    "x_train = np.genfromtxt(\"/Users/innocent/data/clusterShape.csv\", delimiter=\" \",\\\n",
    "                       skip_header=1, dtype='<f4',\n",
    "                       usecols=(n.index('isBarrel'), n.index('layer'), n.index('x'),n.index('y'),\n",
    "#                                n.index('xx'), n.index('yy'), n.index('xy'),\n",
    "                                n.index('dx'), n.index('dy'), n.index('l2'),\n",
    "                                n.index('sx'), n.index('sy'), n.index('s'), n.index('q')\n",
    "                                )\n",
    "                       )\n",
    "x_train = np.nan_to_num(x_train)\n",
    "y_train = np.genfromtxt(\"/Users/innocent/data/clusterShape.csv\", delimiter=\" \",\\\n",
    "                       skip_header=1,dtype='<f4',\n",
    "                       usecols=(n.index('simSX'), n.index('simSY'))\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('isBarrel', 'layer', 'simX', 'simY', 'simSX', 'simSY', 'recX', 'recY', 'x', 'y', 'xx', 'yy', 'xy', 'dx', 'dy', 'l2', 'sx', 'sy', 's', 'q')\n",
      "382361\n"
     ]
    }
   ],
   "source": [
    "print clus.dtype.names\n",
    "print clus.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "(382361, 11)\n",
      "None\n",
      "4205971\n",
      "[1.00000e+00 1.00000e+00 1.71441e-01 1.18207e+00 9.95017e-01 2.72549e+00\n",
      " 1.20569e+00 2.00000e+00 3.00000e+00 4.00000e+00 3.44200e+04]\n",
      "None\n",
      "(382361, 2)\n",
      "4205971\n",
      "[-0.376981  2.08425 ]\n"
     ]
    }
   ],
   "source": [
    "print x_train.dtype\n",
    "print x_train.shape\n",
    "print y_train.dtype.names\n",
    "print x_train.size\n",
    "print x_train[0]\n",
    "\n",
    "print y_train.dtype.names\n",
    "print y_train.shape\n",
    "print x_train.size\n",
    "print y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_ = np.ones(x_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#/////Better metrics to monitor while training\n",
    "#/////////////////////////////////////////////\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print \" — val_f1: %f — val_precision: %f — val_recall %f \" %(_val_f1, _val_precision, _val_recall)\n",
    "        return\n",
    " \n",
    "metrics_ = Metrics()\n",
    "\n",
    "#/////Creating the model\n",
    "#//////////////////////\n",
    "dropoutRate_=0.1\n",
    "reg_rate_=0.0\n",
    "loss_ = 'mse' #'mse','binary_crossentropy','mae'\n",
    "Nepoch_=40\n",
    "batch_size_=512\n",
    "adam_=optimizers.Adam(lr=0.001)\n",
    "reduce_lr_ = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=7, min_lr=0.00001)\n",
    "\n",
    "\n",
    "a_inp = Input(shape=(x_train.shape[1],),name='ins')\n",
    "a = Dense(256,activation='relu', kernel_initializer='normal',kernel_regularizer=l2(reg_rate_))(a_inp)\n",
    "a = Dropout(dropoutRate_)(a)\n",
    "a = Dense(128,activation='relu', kernel_initializer='normal',kernel_regularizer=l2(reg_rate_))(a)\n",
    "a = Dropout(dropoutRate_)(a)\n",
    "a = Dense(32,activation='relu', kernel_initializer='normal',kernel_regularizer=l2(reg_rate_))(a)\n",
    "a = Dropout(dropoutRate_)(a)\n",
    "a = Dense(16,activation='relu', kernel_initializer='normal',kernel_regularizer=l2(reg_rate_))(a)\n",
    "a_out = Dense(2, activation='linear', kernel_initializer='normal',name='outs')(a)\n",
    "\n",
    "model=Model(inputs=a_inp,outputs=a_out)\n",
    "model.compile(loss=loss_, optimizer=adam_,metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 344124 samples, validate on 38237 samples\n",
      "Epoch 1/40\n",
      "344124/344124 [==============================] - 3s 10us/step - loss: 2.0371 - acc: 0.7691 - val_loss: 1.1816 - val_acc: 0.7733\n",
      "Epoch 2/40\n",
      "344124/344124 [==============================] - 3s 8us/step - loss: 1.1303 - acc: 0.7738 - val_loss: 1.2543 - val_acc: 0.7733\n",
      "Epoch 3/40\n",
      "344124/344124 [==============================] - 3s 9us/step - loss: 1.0645 - acc: 0.7739 - val_loss: 1.1742 - val_acc: 0.7729\n",
      "Epoch 4/40\n",
      "344124/344124 [==============================] - 4s 11us/step - loss: 1.0395 - acc: 0.7738 - val_loss: 1.3085 - val_acc: 0.7709\n",
      "Epoch 5/40\n",
      "344124/344124 [==============================] - 3s 10us/step - loss: 1.0330 - acc: 0.7737 - val_loss: 1.1573 - val_acc: 0.7717\n",
      "Epoch 6/40\n",
      "344124/344124 [==============================] - 3s 9us/step - loss: 1.0284 - acc: 0.7737 - val_loss: 1.4009 - val_acc: 0.7686\n",
      "Epoch 7/40\n",
      "344124/344124 [==============================] - 3s 9us/step - loss: 1.0227 - acc: 0.7738 - val_loss: 1.4500 - val_acc: 0.7669\n",
      "Epoch 8/40\n",
      "344124/344124 [==============================] - 3s 9us/step - loss: 1.0109 - acc: 0.7737 - val_loss: 1.1871 - val_acc: 0.7706\n",
      "Epoch 9/40\n",
      "344124/344124 [==============================] - 3s 9us/step - loss: 0.9850 - acc: 0.7738 - val_loss: 1.5110 - val_acc: 0.7666\n",
      "Epoch 10/40\n",
      "344124/344124 [==============================] - 3s 9us/step - loss: 0.8066 - acc: 0.7729 - val_loss: 0.5444 - val_acc: 0.7679\n",
      "Epoch 11/40\n",
      "344124/344124 [==============================] - 3s 9us/step - loss: 0.5786 - acc: 0.7720 - val_loss: 0.5763 - val_acc: 0.7694\n",
      "Epoch 12/40\n",
      "344124/344124 [==============================] - 3s 9us/step - loss: 0.5372 - acc: 0.7736 - val_loss: 0.5591 - val_acc: 0.7723\n",
      "Epoch 13/40\n",
      "178688/344124 [==============>...............] - ETA: 1s - loss: 0.5295 - acc: 0.7738"
     ]
    }
   ],
   "source": [
    "#//////Train the model\n",
    "#/////////////////////\n",
    "\n",
    "#NOTE: Due to some unfortunate incompability with the notebooks and keras,\n",
    "#displaying the progress of training requires additional effort, i.e.\n",
    "#TQDMNotebookCallback or something similar. This however interferes with\n",
    "#printing the custom monitoring metrics at the end of epoch. They will\n",
    "#show correctly after the training finishes.\n",
    "\n",
    "history=model.fit(x_train,y_train,\n",
    "        epochs=Nepoch_,\n",
    "        batch_size=batch_size_,\n",
    "        sample_weight=weights_,\n",
    "        callbacks=[reduce_lr_],\n",
    "#        callbacks=[reduce_lr_,metrics_,TQDMNotebookCallback(metric_format=\"{name}: {value:0.6f}\")],\n",
    "        validation_split=0.1,\n",
    "        shuffle=True,\n",
    "        verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train loss', 'test loss','test f1'])\n",
    "plt.show()\n",
    "plt.savefig('Training_losses.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.plot(metrics_.val_f1s)\n",
    "#plt.plot(metrics_.val_recalls)\n",
    "#plt.plot(metrics_.val_precisions)\n",
    "#plt.title('Metrics')\n",
    "#plt.ylabel('Value')\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.legend(['F1','Recall','Precision'])\n",
    "#plt.savefig('Metrics.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_train)\n",
    "print y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb = np.ma.masked_where(clus['isBarrel']==0,clus['layer']+1)\n",
    "le = np.ma.masked_where(clus['isBarrel']==1,clus['layer']+1)\n",
    "# nn = np.ma.masked_where(np.logical_or(clus['simSY']<5,clus['dy']<clus['simSY']),clus['layer']+1)\n",
    "plt.scatter(y_pred[:,1],y_train[:,1],s=lb,marker='o',c=clus['layer'])\n",
    "plt.show()\n",
    "plt.scatter(y_pred[:,0],y_train[:,0],s=lb,marker='o',c=clus['layer'])\n",
    "plt.show()\n",
    "plt.scatter(y_pred[:,1],y_train[:,1],s=le,marker='o',c=clus['layer'])\n",
    "plt.show()\n",
    "plt.scatter(y_pred[:,0],y_train[:,0],s=le,marker='o',c=clus['layer'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hist(w) :\n",
    "    nbins = 100\n",
    "    y1,xy = np.histogram(y_pred[:,1]-y_train[:,1],np.linspace(-4.,4.,nbins),weights=w, density=True)\n",
    "    y2,xy = np.histogram(y_pred[:,0]-y_train[:,0],np.linspace(-4.,4.,nbins),weights=w, density=True)\n",
    "    xy.resize(len(y2))\n",
    "    return xy,y1,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isS2 = np.equal(clus['sx'],2)*np.greater(clus['x'],0.5)\n",
    "isl1 = np.equal(clus['layer'],1)\n",
    "notl1 = np.logical_not(isl1)\n",
    "notS1 = np.greater(clus['s'],1)\n",
    "xy,y1,y2 = hist(clus['isBarrel']*notS1) # *isl1)\n",
    "exy,ey1,ey2 = hist((1-clus['isBarrel'])*notS1)  # *isl1)                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.step(xy,y1,where='post')\n",
    "plt.step(xy,y2,where='post')\n",
    "plt.show()\n",
    "plt.step(exy,ey1,where='post')\n",
    "plt.step(exy,ey2,where='post')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
